# Phase 4.1 â€” Behavioral Intelligence Validation Checklist

**Release**: v4.1.0 "Behavioral Intelligence"  
**Date**: 2025-01-XX  
**Status**: â³ Pending QA Sign-Off

## Overview

This checklist validates two critical behavioral fixes:
1. **Social greetings** â†’ DAC persona (no dictionary definitions, no citations)
2. **Time-sensitive queries** â†’ Real-time multi-search with synthesis

---

## âœ… Test 1: Greeting Flow

### Test Case 1.1: Basic Greeting
**Input**: `hi` or `hello there`  
**Expected**:
- âœ… Friendly DAC greeting (1-2 sentences)
- âœ… No citations `[1][2][3]`
- âœ… No dictionary definitions
- âœ… Optional light follow-up question
- âœ… Intent: `social_chat`
- âœ… Provider: `openai` / Model: `gpt-4o-mini`
- âœ… Pipeline: `direct_llm` (NOT `web_multisearch`)

**Logs to Verify**:
```json
{
  "intent": "social_chat",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "pipeline": "direct_llm",
  "behavior": "chat_only"
}
```

**Pass/Fail**: â¬œ

---

### Test Case 1.2: Conversational Follow-up
**Input**: `how are you?`  
**Expected**:
- âœ… Warm, conversational response (1-2 sentences)
- âœ… Maintains DAC tone
- âœ… No citations
- âœ… Intent: `social_chat`

**Pass/Fail**: â¬œ

---

### Test Case 1.3: Context Switch (Definition Request)
**Input**: `define "hi there"`  
**Expected**:
- âœ… Definition provided (user explicitly asked)
- âœ… Citation included (definition request = factual query)
- âœ… Intent switches to `qa_retrieval`
- âœ… Provider may change to Perplexity

**Pass/Fail**: â¬œ

**Notes**: This confirms the system correctly distinguishes between greeting and definition requests.

---

## âœ… Test 2: Real-Time Multi-Search

### Test Case 2.1: Time-Sensitive Query (Place + Time)
**Input**: `what happened in delhi india two days ago`  
**Expected**:
- âœ… Multi-source summary (3-6 bullet points)
- âœ… Dates included in bullets
- âœ… Short recency note if coverage is limited
- âœ… Citations list not empty (logged)
- âœ… Intent: `qa_retrieval`
- âœ… Pipeline: `web_multisearch`
- âœ… Provider: `web+openai`

**Logs to Verify**:
```json
{
  "intent": "qa_retrieval",
  "pipeline": "web_multisearch",
  "provider": "web+openai",
  "model": "gpt-4o-mini",
  "citations": ["url1", "url2", ...],
  "fallback_used": false
}
```

**Pass/Fail**: â¬œ

---

### Test Case 2.2: Time-Sensitive Query (Topic + Time)
**Input**: `what's new in ai this week`  
**Expected**:
- âœ… Same multi-search path triggered
- âœ… Aggregated summary from multiple sources
- âœ… Recent events (last 7 days)
- âœ… Citations present

**Pass/Fail**: â¬œ

---

### Test Case 2.3: Non-Time-Sensitive Retrieval (Control)
**Input**: `who is the chief minister of delhi`  
**Expected**:
- âœ… Falls back to `direct_llm` pipeline
- âœ… NOT `web_multisearch` (no time indicators)
- âœ… Standard retrieval answer
- âœ… May use Perplexity or other provider

**Logs to Verify**:
```json
{
  "intent": "qa_retrieval",
  "pipeline": "direct_llm",
  "provider": "perplexity" // or other
}
```

**Pass/Fail**: â¬œ

**Notes**: Confirms time-sensitive detection doesn't trigger false positives.

---

## âœ… Test 3: Tone Consistency (Long Conversation)

### Test Case 3.1: Multi-Turn Conversation
**Sequence**:
1. User: `hi there`
2. User: `can you code a function that prints today's date?`
3. User: `explain it`
4. User: `btw what happened in delhi india two days ago`

**Expected**:
- âœ… Turn 1: Friendly greeting (social_chat, OpenAI)
- âœ… Turn 2: Code generation (coding_help, Gemini/OpenAI)
- âœ… Turn 3: Explanation (qa_retrieval, direct_llm)
- âœ… Turn 4: Multi-search summary (qa_retrieval, web_multisearch)
- âœ… Each response maintains DAC tone
- âœ… Context remembered across turns
- âœ… No provider/model names exposed

**Pass/Fail**: â¬œ

**Notes**: Verify memory/context continuity in logs.

---

## âœ… Test 4: Regression Tests

### Test Case 4.1: Editing/Writing Intent
**Input**: `edit this email: hey team we shipped!`  
**Expected**:
- âœ… Intent: `editing/writing`
- âœ… Improved text returned
- âœ… Key changes noted
- âœ… No behavioral regression

**Pass/Fail**: â¬œ

---

### Test Case 4.2: Reasoning/Math Intent
**Input**: `solve: 2x + 5 = 11`  
**Expected**:
- âœ… Intent: `reasoning/math`
- âœ… Step-by-step logic shown
- âœ… Final answer provided
- âœ… Provider: OpenAI (reasoning specialist)

**Pass/Fail**: â¬œ

---

### Test Case 4.3: Coding Help Intent
**Input**: `make this code faster` (with code context)  
**Expected**:
- âœ… Intent: `coding_help`
- âœ… Optimized code provided
- âœ… Explanation included
- âœ… Provider: Gemini or OpenAI

**Pass/Fail**: â¬œ

---

### Test Case 4.4: Context Recall
**Input**: `what were we working on again?` (after previous conversation)  
**Expected**:
- âœ… Recalls previous context (e.g., "Alex" + "Python project")
- âœ… 1-2 sentence summary
- âœ… Intent: `social_chat` or `qa_retrieval` (context-dependent)

**Pass/Fail**: â¬œ

---

## âœ… Test 5: Observability & Logging

### Test Case 5.1: Log Structure
**Check**: `observability.log_turn()` output for time-sensitive query

**Expected Fields**:
```json
{
  "intent": "qa_retrieval",
  "provider": "web+openai",
  "model": "gpt-4o-mini",
  "latency_ms": <numeric_value>,
  "cache_hit": false,
  "fallback_used": false,
  "safety_flags": [],
  "pipeline": "web_multisearch",
  "citations": ["url1", "url2"]
}
```

**Pass/Fail**: â¬œ

---

### Test Case 5.2: OTEL Spans
**Check**: OpenTelemetry spans include:
- âœ… `dac.intent` attribute
- âœ… `dac.provider` attribute
- âœ… `dac.pipeline` attribute (for web_multisearch)

**Pass/Fail**: â¬œ

---

### Test Case 5.3: Grafana Metrics
**Check**: Grafana dashboard shows:
- âœ… New intent: `social_chat` (count > 0)
- âœ… New pipeline: `web_multisearch` (count > 0)
- âœ… Provider breakdown includes `web+openai`

**Pass/Fail**: â¬œ

---

## âœ… Test 6: Performance & Cost

### Test Case 6.1: Latency
**Expected**:
- âœ… Greeting response: < 2 seconds
- âœ… Time-sensitive multi-search: < 8 seconds
- âœ… Non-time-sensitive retrieval: < 3 seconds

**Pass/Fail**: â¬œ

---

### Test Case 6.2: Cost Per Turn
**Expected**:
- âœ… Greeting: < $0.001 (OpenAI mini, ~50 tokens)
- âœ… Multi-search: < $0.01 (Perplexity search + OpenAI synthesis)
- âœ… Average cost per turn: < $0.005

**Pass/Fail**: â¬œ

**Notes**: Multi-search adds 1 search call + 1 synthesis call, but should remain cost-effective.

---

## ğŸš€ Pre-Deployment Checklist

- [ ] All test cases above pass
- [ ] No linter errors
- [ ] No new exceptions in logs
- [ ] Grafana metrics look healthy
- [ ] Cost metrics within budget
- [ ] Performance metrics acceptable

---

## ğŸ“‹ Deployment Steps

1. **Run Post-Deploy Sanity**:
   ```bash
   ./scripts/post_deploy_sanity.sh
   ```

2. **Monitor Grafana**:
   - Watch for new intents (`social_chat`, `qa_retrieval:web_multisearch`)
   - Check error rates
   - Monitor latency percentiles

3. **Cost Verification**:
   - Confirm cost per turn < $0.01
   - Track multi-search usage
   - Verify no cost spikes

4. **Stability Period**:
   - Monitor for 24 hours
   - Check error logs
   - Verify user feedback

5. **Merge & Tag**:
   ```bash
   git tag v4.1.0-behavioral-intelligence
   git push origin v4.1.0-behavioral-intelligence
   ```

---

## ğŸ“ Sign-Off

**QA Engineer**: _________________ **Date**: ________  
**Engineering Lead**: _________________ **Date**: ________  
**Product Owner**: _________________ **Date**: ________

---

## ğŸ› Known Issues / Notes

_Add any discovered issues or notes here during testing._

---

## âœ… Success Criteria

Phase 4.1 is considered successful when:

1. âœ… Greetings feel natural and conversational (no dictionary definitions)
2. âœ… Time-sensitive queries return real-time, multi-source summaries
3. âœ… DAC maintains consistent personality across all intents
4. âœ… No regression in existing functionality
5. âœ… Observability provides clear visibility into routing decisions
6. âœ… Cost and performance remain within acceptable bounds

---

**End of Validation Checklist**

